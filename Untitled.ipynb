{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from util.define import logger\n",
    "DATASET_PATH = \"E:/한국어 음성데이터/KaiSpeech/\"\n",
    "TRAIN_LIST_PATH = \"./data/data_list/train_list.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_list(data_list_path, dataset_path):\n",
    "    data_list = pd.read_csv(data_list_path, \"r\", delimiter = \",\", encoding=\"cp949\")\n",
    "    audio_paths = list(dataset_path + data_list[\"audio\"])\n",
    "    label_paths = list(dataset_path + data_list[\"label\"])\n",
    "\n",
    "    return audio_paths, label_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:/한국어 음성데이터/KaiSpeech/KaiSpeech_103762.pcm',\n",
       " 'E:/한국어 음성데이터/KaiSpeech/KaiSpeech_126833.pcm',\n",
       " 'E:/한국어 음성데이터/KaiSpeech/KaiSpeech_367790.pcm',\n",
       " 'E:/한국어 음성데이터/KaiSpeech/KaiSpeech_005465.pcm',\n",
       " 'E:/한국어 음성데이터/KaiSpeech/KaiSpeech_133374.pcm']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_paths, label_paths = load_data_list(data_list_path=TRAIN_LIST_PATH, dataset_path=DATASET_PATH)\n",
    "audio_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_librosa_mfcc(filepath = None, n_mfcc = 33, del_silence = False, input_reverse = True, format='pcm'):\n",
    "    if format == 'pcm':\n",
    "        try:\n",
    "            pcm = np.memmap(filepath, dtype='h', mode='r')\n",
    "        except: # exception handling\n",
    "            logger.info(\"np.memmap error in %s\" % filepath)\n",
    "            return torch.zeros(1)\n",
    "        sig = np.array([float(x) for x in pcm])\n",
    "    elif format == 'wav':\n",
    "        sig, _ = librosa.core.load(filepath, sr=16000)\n",
    "    else: logger.info(\"%s is not Supported\" % format)\n",
    "\n",
    "    if del_silence:\n",
    "        non_silence_indices = librosa.effects.split(sig, top_db=30)\n",
    "        sig = np.concatenate([sig[start:end] for start, end in non_silence_indices])\n",
    "    feat = librosa.feature.mfcc(y=sig,sr=16000, hop_length=160, n_mfcc=n_mfcc, n_fft=400, window='hamming')\n",
    "    if input_reverse:\n",
    "        feat = feat[:,::-1]\n",
    "\n",
    "    return torch.FloatTensor( np.ascontiguousarray( np.swapaxes(feat, 0, 1) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_augment(feat, T=40, F=15, time_mask_num=2, freq_mask_num=2):\n",
    "    feat_size = feat.size(1)\n",
    "    seq_len = feat.size(0)\n",
    "\n",
    "    # time mask\n",
    "    for _ in range(time_mask_num):\n",
    "        t = np.random.uniform(low=0.0, high=T)\n",
    "        t = int(t)\n",
    "        t0 = random.randint(0, seq_len - t)\n",
    "        feat[t0 : t0 + t, :] = 0\n",
    "\n",
    "    # freq mask\n",
    "    for _ in range(freq_mask_num):\n",
    "        f = np.random.uniform(low=0.0, high=F)\n",
    "        f = int(f)\n",
    "        f0 = random.randint(0, feat_size - f)\n",
    "        feat[:, f0 : f0 + f] = 0\n",
    "\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc_pickle(audio_paths):\n",
    "    mfcc_list = []\n",
    "    for idx in trange(len(audio_paths)):\n",
    "        mfcc_list.append(get_librosa_mfcc(filepath=audio_paths[idx], \n",
    "                                          n_mfcc=33,\n",
    "                                          del_silence=False,\n",
    "                                          input_reverse=True,\n",
    "                                          format=\"pcm\"))\n",
    "    with open(\"./data/pickle/mfccs.txt\", \"wb\") as f:\n",
    "        pickle.dump(mfcc_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augment_pickle(audio_paths):\n",
    "    augment_list = []\n",
    "    for idx in trange(len(audio_paths)):\n",
    "        feat = get_librosa_mfcc(filepath=audio_paths[idx], \n",
    "                                          n_mfcc=33,\n",
    "                                          del_silence=False,\n",
    "                                          input_reverse=True,\n",
    "                                          format=\"pcm\")\n",
    "        augment_list.append(spec_augment(feat, T=40, F=15, time_mask_num=2, freq_mask_num=2))\n",
    "    with open(\"./data/pickle/augments.txt\", \"wb\") as f:\n",
    "        pickle.dump(augment_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610591"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "target_dict = None\n",
    "with open(\"./data/pickle/target_dict.txt\", \"rb\") as f:\n",
    "    target_dict = pickle.load(f)\n",
    "len(target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.dataset import BaseDataset\n",
    "from util.define import SOS_token, EOS_token\n",
    "\n",
    "def split_dataset(hparams, audio_paths, label_paths, valid_ratio=0.05, target_dict = dict()):\n",
    "    # audio_paths & label_paths shuffled in the same order\n",
    "    # for seperating train & validation\n",
    "    train_num = math.ceil(len(audio_paths) * (1 - valid_ratio))\n",
    "    data_paths = list(zip(audio_paths, label_paths))\n",
    "    random.shuffle(data_paths)\n",
    "    audio_paths, label_paths = zip(*data_paths)\n",
    "\n",
    "    # seperating the train dataset by the number of workers\n",
    "    train_dataset_list.append(BaseDataset(audio_paths=audio_paths[:train_num],\n",
    "                                          label_paths=label_paths[:train_num],\n",
    "                                          bos_id=SOS_token, eos_id=EOS_token, target_dict=target_dict,\n",
    "                                          input_reverse=True, use_augment=True))\n",
    "\n",
    "    valid_dataset = BaseDataset(audio_paths=audio_paths[train_num:],\n",
    "                                label_paths=label_paths[train_num:],\n",
    "                                bos_id=SOS_token, eos_id=EOS_token,\n",
    "                                target_dict=target_dict, input_reverse=hparams.input_reverse, use_augment=False)\n",
    "\n",
    "    #save_pickle(train_dataset_list, \"./pickle/train_dataset.txt\", \"dump all train_dataset_list using pickle complete !!\")\n",
    "    #save_pickle(valid_dataset, \"./pickle/valid_dataset.txt\", \"dump all valid_dataset using pickle complete !!\")\n",
    "    logger.info(\"split dataset complete !!\")\n",
    "    return train_time_step, train_dataset_list, valid_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
